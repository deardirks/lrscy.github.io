<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    
<!-- Google Analytics -->
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'true', 'auto');
ga('send', 'pageview');
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<!-- End Google Analytics -->


    

    



    <meta charset="utf-8">
    
    <meta name="google-site-verification" content="true">
    
    
    
    <title>DeepLearning.ai Note - Neural Network and Deep Learning | Meow</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Deep Learning,Coursera">
    <meta name="description" content="This is a note of the first course of the “Deep Learning Specialization” at Coursera. The course is taught by Andrew Ng. Almost all materials in this note come from courses’ videos. The note combines">
<meta name="keywords" content="Deep Learning,Coursera">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearning.ai Note - Neural Network and Deep Learning">
<meta property="og:url" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/index.html">
<meta property="og:site_name" content="Meow">
<meta property="og:description" content="This is a note of the first course of the “Deep Learning Specialization” at Coursera. The course is taught by Andrew Ng. Almost all materials in this note come from courses’ videos. The note combines">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Standard_NN.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Convolutional_NN.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Recurrent_NN.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Structured_and_Unstructured_Data.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Comparation_between_deep_learning_and_machine_learning.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Iteration_process.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Standard_notations.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Standard_representations.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Input_X.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Output_y.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Logistic_Regression.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Neural_Network.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Computation_Graph.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Computing_Derivatives.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Logistic_Regression.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Forward_Propagation_NN.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Backward_Propagation_NN.png">
<meta property="og:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Activation_Functions.png">
<meta property="og:updated_time" content="2018-10-31T03:54:43.843Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepLearning.ai Note - Neural Network and Deep Learning">
<meta name="twitter:description" content="This is a note of the first course of the “Deep Learning Specialization” at Coursera. The course is taught by Andrew Ng. Almost all materials in this note come from courses’ videos. The note combines">
<meta name="twitter:image" content="https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/Standard_NN.png">
    
        <link rel="alternate" type="application/atom+xml" title="Meow" href="../../../../atom.xml">
    
    <link rel="shortcut icon" href="../../../../favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(../../../../img/brand.jpg)">
      <div class="brand">
        <a href="../../../../index.html" class="avatar waves-effect waves-circle waves-light">
          <img src="../../../../img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Meow</h5>
          <a href="mailto:ruosenlee@gmail.com" title="ruosenlee@gmail.com" class="mail">ruosenlee@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="../../../../index.html">
                <i class="icon icon-lg icon-home"></i>
                Homepage
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="../../../../archives">
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="../../../../tags">
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="../../../../categories">
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="../../../../links">
                <i class="icon icon-lg icon-link"></i>
                Links
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/lrscy" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">DeepLearning.ai Note - Neural Network and Deep Learning</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">DeepLearning.ai Note - Neural Network and Deep Learning</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-10-22T16:32:25.000Z" itemprop="datePublished" class="page-time">
  2018-10-22
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="../../../../categories/Deep-Learning/">Deep Learning</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Brief-Intro-to-Deep-Learning"><span class="post-toc-number">1.</span> <span class="post-toc-text">Brief Intro to Deep Learning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Stuctures-of-Deep-Learning"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Stuctures of Deep Learning</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Data"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">Data</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Advantages"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">Advantages</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Basic-Symbols-of-the-Course"><span class="post-toc-number">2.</span> <span class="post-toc-text">Basic Symbols of the Course</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Neural-Network"><span class="post-toc-number">3.</span> <span class="post-toc-text">Neural Network</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Logistic-Regression-and-Neural-Network"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">Logistic Regression and Neural Network</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Computation-Graph"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">Computation Graph</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Forward-Propagation"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Forward Propagation</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Computation-on-single-neuron"><span class="post-toc-number">3.3.1.</span> <span class="post-toc-text">Computation on single neuron</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#The-whole-process-on-Neural-Network"><span class="post-toc-number">3.3.2.</span> <span class="post-toc-text">The whole process on Neural Network</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Cost-function"><span class="post-toc-number">3.3.3.</span> <span class="post-toc-text">Cost function</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Backward-Propagation"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">Backward Propagation</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Compute-Gradients"><span class="post-toc-number">3.4.1.</span> <span class="post-toc-text">Compute Gradients</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Update-parameters"><span class="post-toc-number">3.4.2.</span> <span class="post-toc-text">Update parameters</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Activation-Functions"><span class="post-toc-number">3.5.</span> <span class="post-toc-text">Activation Functions</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Parameters-and-Hyperparameters"><span class="post-toc-number">3.6.</span> <span class="post-toc-text">Parameters and Hyperparameters</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Comparation-of-shallow-and-deep-neural-network"><span class="post-toc-number">3.7.</span> <span class="post-toc-text">Comparation of shallow and deep neural network</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-DeepLearningNotes-NNandDL" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">DeepLearning.ai Note - Neural Network and Deep Learning</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-10-22 12:32:25" datetime="2018-10-22T16:32:25.000Z" itemprop="datePublished">2018-10-22</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="../../../../categories/Deep-Learning/">Deep Learning</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>This is a note of the first course of the “Deep Learning Specialization” at <a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Coursera</a>. The course is taught by Andrew Ng.</p>
<p>Almost all materials in this note come from courses’ videos. The note combines knowledge from course and some of my understanding of these konwledge. I’ve reorganized the structure of the whole course according to my understanding. Thus, it doesn’t strictly follow the order of videos.</p>
<p>In this note, I will keep all functions and equations vectorized (without for loop) as far as possible.</p>
<p>If you want to read the notes which strictly follows the course, here are some recommendations:</p>
<ul>
<li><a href="https://github.com/mbadry1/DeepLearning.ai-Summary" target="_blank" rel="noopener">mbadry1’s notes on Github</a></li>
<li><a href="https://github.com/ppant/deeplearning.ai-notes" target="_blank" rel="noopener">ppant’s notes on Github</a></li>
</ul>
<p>Some parts of this note are inspired from <a href="https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng" target="_blank" rel="noopener">Tess Ferrandez</a>.</p>
<h1 id="Brief-Intro-to-Deep-Learning"><a href="#Brief-Intro-to-Deep-Learning" class="headerlink" title="Brief Intro to Deep Learning"></a>Brief Intro to Deep Learning</h1><p>To begin with, let’s focus on some basic concepts to gain some intuition of deep learning.</p>
<h2 id="Stuctures-of-Deep-Learning"><a href="#Stuctures-of-Deep-Learning" class="headerlink" title="Stuctures of Deep Learning"></a>Stuctures of Deep Learning</h2><p>We start with supervised learning. Here are several types of neural network (NN) in the folloing chart:</p>
<table>
<thead>
<tr>
<th style="text-align:center">INPUT: X</th>
<th style="text-align:center">OUTPUT: y</th>
<th style="text-align:center">NN TYPE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Home features</td>
<td style="text-align:center">Price</td>
<td style="text-align:center">Standard NN</td>
</tr>
<tr>
<td style="text-align:center">Ad, user info</td>
<td style="text-align:center">Click or not</td>
<td style="text-align:center">Standard NN</td>
</tr>
<tr>
<td style="text-align:center">Image</td>
<td style="text-align:center">Objects</td>
<td style="text-align:center">Convolutional NN (CNN)</td>
</tr>
<tr>
<td style="text-align:center">Audio</td>
<td style="text-align:center">Text Transcription</td>
<td style="text-align:center">Recurrent NN (RNN)</td>
</tr>
<tr>
<td style="text-align:center">English</td>
<td style="text-align:center">Chineses</td>
<td style="text-align:center">Recurrent NN (RNN)</td>
</tr>
<tr>
<td style="text-align:center">Image, Radar info</td>
<td style="text-align:center">Position of other cars</td>
<td style="text-align:center">Custom NN</td>
</tr>
</tbody>
</table>
<p>Here are some pictures of Standard NN, Convolutional NN, Recurrent NN: </p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Standard_NN.png" height="70%" width="70%"><br>  <div class="image-caption">Standard NN</div><br>  <img src="/2018/10/22/DeepLearningNotes-NNandDL/Convolutional_NN.png" height="70%" width="70%"><br>  <div class="image-caption">Convolutional NN</div><br>  <img src="/2018/10/22/DeepLearningNotes-NNandDL/Recurrent_NN.png" height="70%" width="70%"><br>  <div class="image-caption">Recurrent NN</div><br></div>

<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Neural Nework can deal with both stuctured data and unstructured data. The following will give you an intuition of both kinds of data.</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Structured_and_Unstructured_Data.png" height="50%" width="50%"><br>  <div class="image-caption">Structured and Unstructured Data</div><br></div>

<h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><p>Here are some conclusions of why deep learning is advanced comparing to traditional machine learning.</p>
<p>Firstly, deep learning models performs better when dealing with big data. Here is a comparation of deep learning models and classic machine learing models:</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Comparation_between_deep_learning_and_machine_learning.png"><br>  <div class="image-caption">Comparation between deep learning and machine learning</div><br></div>

<p>Secondly, thanks to the booming development of hardware and advanced algorithm, computing is much faster than before. Thus, we can implement our idea and know whether it works or not in short time. As a result, we can run the following circle much faster than we image.</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Iteration_process.png" height="50%" width="50%"><br>  <div class="image-caption">Iteration process</div><br></div>

<h1 id="Basic-Symbols-of-the-Course"><a href="#Basic-Symbols-of-the-Course" class="headerlink" title="Basic Symbols of the Course"></a>Basic Symbols of the Course</h1><p>These basic symbols will be used through out the whole specialization.</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Standard_notations.png"><br>  <div class="image-caption">Standard notations</div><br>  <img src="/2018/10/22/DeepLearningNotes-NNandDL/Standard_representations.png"><br>  <div class="image-caption">Standard representation</div><br></div>

<p>Moreover, in this course, each input x will be stacked into columns and form the input matrix X.</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Input_X.png" width="50%" height="50%"><br>  <div class="image-caption">Input X</div><br>  <img src="/2018/10/22/DeepLearningNotes-NNandDL/Output_y.png" width="50%" height="50%"><br>  <div class="image-caption">Output y</div><br></div>

<h1 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h1><p>Reviewing the whole course, there are several common concepts between logistic regression and neural network (including both shallow and deep neural network). Thus, I draw conclusions on each concept and then apply them to both logistic regression and neural network.</p>
<h2 id="Logistic-Regression-and-Neural-Network"><a href="#Logistic-Regression-and-Neural-Network" class="headerlink" title="Logistic Regression and Neural Network"></a>Logistic Regression and Neural Network</h2><p>First of all, here are pictures of logistic regression and neural network.</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Logistic_Regression.png" width="60%" height="60%"><br>  <div class="image-caption">Logistic Regression</div><br>  <img src="/2018/10/22/DeepLearningNotes-NNandDL/Neural_Network.png" width="60%" height="60%"><br>  <div class="image-caption">Neural Network</div><br></div>

<p>As we can see, logistic regression is also a kind of neural network, which has input layer and output layer and does not have hidden layers, so that it is also called mini neural network. In the following sections, I will write “neural network” to represent logistic regression and neural network and use pictures similar to the second one to represent neural network.</p>
<h2 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a>Computation Graph</h2><p>Computation graph is one of basic concepts in deep learning. By analyzing it, we could understand the whole process of computation process of neural network. The following is the basic computation graph:</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Computation_Graph.png" width="80%" height="80%"></div>

<p>In this picture, we can easily understand how $J(a,b,c)=3(a+bc)$ is computed. This process is similar to “Forward Propagation” process which I will say in next section. Moreover, in neural network, $J$ is called cost function. After computing cost function $J$, we need to feed it back to all of our parameters, such as $a$, $b$, $c$ in the picture. This process is called computing derivatives.</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Computing_Derivatives.png" width="80%" height="80%"></div>

<p>By analyzing the comutation graph, we can easily compute all deviatives. According to chain rule, we can compute $\frac{dJ}{da}$ by $\frac{dJ}{dv}\frac{dv}{da}$. So do parameter $b$ and $c$. The whole derivation process is similar to “backward propagation” process in neural network.</p>
<h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><h3 id="Computation-on-single-neuron"><a href="#Computation-on-single-neuron" class="headerlink" title="Computation on single neuron"></a>Computation on single neuron</h3><div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Logistic_Regression.png" width="60%" height="60%"><br>  <div class="image-caption">Computation on single neuron</div><br></div>

<p>For every single neuron, the computing process is the same as the logistic regression. Logistic regression is basically the combination of linear regression and logistic function such as sigmoid. It has one input layer, x, and one output layer, a or $ \hat{y} $.</p>
<p>The linear regression equation is:&ensp;$ z = w^Tx+b $<br>The sigmoid function equation is:&ensp;$ a = \sigma( z ) $<br>The combination euquation is:&emsp;&ensp;&nbsp;$ \hat{y} = a = \sigma( w^Tx + b ) $</p>
<h3 id="The-whole-process-on-Neural-Network"><a href="#The-whole-process-on-Neural-Network" class="headerlink" title="The whole process on Neural Network"></a>The whole process on Neural Network</h3><div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Forward_Propagation_NN.png" width="90%" height="90%"><br>  <div class="image-caption">Forward Propagation</div><br></div>

<p>This is an example of neural network. Since it only has one hidden layer, it’s also called shallow neural network. The forward propagation process means that we compute the graph from left to the right in this picture.</p>
<p>The whole process when computing the 1<sup>st</sup> layer (hidden layer) is as the following:</p>
<p>\begin{align}<br>Z^{[1]} &amp; = W^{[1]T}X + b^{[1]} \\<br>A^{[1]} &amp; = \sigma( Z^{[1]} )<br>\end{align}</p>
<p>In these equations:</p>
<ul>
<li>$W^{[1]T}$ is a $4 \times 3$ matrix. It is also written as $W^{[1]}$. Its shape is always $n^{[l]} \times n^{[l - 1]}$.</li>
<li>$X$ is a $3 \times m$ matrix. Sometimes it is also called $A^{[0]}$.</li>
<li>$b^{[1]}$ is a $4 \times m$ matrix. Its shape is always $n^{[l]} \times m$.</li>
<li>$A^{[1]}$ is a $4 \times m$ matrix. Its shape is always $n^{[l]} \times m$.</li>
<li>$\sigma$ is an element-wise function. It is called activation function.</li>
</ul>
<p>For each layer, it just repeats what previous layers do until the last layer (output layer).</p>
<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>Here is a definition of loss function and cost function.</p>
<ul>
<li>Loss function computes a single training example.</li>
<li>Cost function is the average of the loss function of the whole training set.</li>
</ul>
<p>In traditional machine learning, we use square root error as loss function, which is $ L = \frac{1}{2}( \hat{y} - y )^2 $. But in this case, we don’t use it since most problems we try to solve are not convex.</p>
<p>Here is the loss function we use:</p>
<p>$$<br>L( \hat{y}, y ) = -( y \cdot log(\hat{y}) + ( 1 - y ) \cdot log( 1 - \hat{y} ) )<br>$$</p>
<p>For this loss function:</p>
<ul>
<li>if y = 1, then $ L = -y \cdot log(\hat{y}) $ and it will close to 0 when $ \hat{y} $ near 1.</li>
<li>if y = 0, then $ L = -( 1 - y ) \cdot log( 1 - \hat{y} ) $ and it will close to 0 when $ \hat{y} $ near 0.</li>
</ul>
<p>Then the cost function is: $$ J( w, b ) = \frac{1}{m}\sum_{i=1}^{m} L( \hat{y}, y ) $$</p>
<h2 id="Backward-Propagation"><a href="#Backward-Propagation" class="headerlink" title="Backward Propagation"></a>Backward Propagation</h2><p>Here, we use gradient descent as our backward propagation method.</p>
<h3 id="Compute-Gradients"><a href="#Compute-Gradients" class="headerlink" title="Compute Gradients"></a>Compute Gradients</h3><div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Backward_Propagation_NN.png" width="90%" height="90%"><br>  <div class="image-caption">Backward Propagation</div><br></div>

<p>As we can see in the picture, it is a simplified computation graph. The neural network is on the right-top, which is almost the same as the neural network we discussing in previous section. Backward Propagation is computing derivatives from the right to the left. By following the backward process, we can get derivatives for all parameters, including $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$.</p>
<p>Here I give a rough derivation example of computing gradients of parameter $W^{[1]}$.</p>
<p>\begin{align}<br>\frac{\partial L}{\partial Z^{[1]}} &amp; = W^{[2]T}\frac{\partial L}{\partial Z^{[2]}} \cdot {\sigma}^{[1]\prime}(Z^{[1]}) \\<br>\frac{dL}{dW^{[1]}} &amp; = \frac{\partial L}{\partial Z^{[1]}}\frac{dZ^{[1]}}{dW^{[1]}} \\<br>                    &amp; = \frac{\partial L}{\partial Z^{[1]}}A^{[0]T} \\<br>\frac{dL}{db^{[1]}} &amp; = \frac{\partial L}{\partial Z^{[1]}}\frac{dZ^{[1]}}{db^{[1]}} \\<br>                    &amp; = \frac{\partial L}{\partial Z^{[1]}}<br>\end{align}</p>
<p>It is similar to compute parameters in other layers. In these equations:</p>
<ul>
<li>$\frac{dL}{dW^{[1]}}$ has the same shape as $W^{[1]}$. So do other layers.</li>
<li>$\frac{dL}{db^{[1]}}$ has the same shape as $b^{[1]}$. So do other layers.</li>
<li>the $\cdot$ in first line is an element-wise product.</li>
</ul>
<h3 id="Update-parameters"><a href="#Update-parameters" class="headerlink" title="Update parameters"></a>Update parameters</h3><p>After computing gradients, we can update our parameters quickly.</p>
<p>For every parameters (Take layer1 as an example):</p>
<p>\begin{align}<br>W^{[1]} &amp; = W^{[1]} - \alpha \frac{dL}{dW^{[1]}} \\<br>b^{[1]} &amp; = b^{[1]} - \alpha \frac{dL}{db^{[1]}}<br>\end{align}</p>
<p>In above equations, $\alpha$ is called learning rate, which we need to determine before training.</p>
<h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>In previous sections, notation $\sigma$ is used to represent activation function. In neural network, there are five common activation functions: Sigmoid, Tanh, ReLU, Leaky ReLU, and Exponential LU.</p>
<div align="center"><img src="/2018/10/22/DeepLearningNotes-NNandDL/Activation_Functions.png" width="90%" height="90%"><br>  <div class="image-caption">Activation Functions</div><br></div>

<p>In the course, Prof. Andrew Ng only introduces the first four activation functions.</p>
<p>Here are some experience on choosing those activation functions:</p>
<ul>
<li>Sigmoid: It is usually used in output layer to generate results between 0 and 1 when doing binary classification. In other case, you should not use it.</li>
<li>Tanh: It always works better than sigmoid function since its value is between -1 and 1, so that neural network can learn more information by using it than using sigmoid function.</li>
<li>ReLU: The most commonly used activation function is ReLU function. If you don’t want to use it, you can choose other ReLU derivatives, such as Leaky ReLU.</li>
</ul>
<h2 id="Parameters-and-Hyperparameters"><a href="#Parameters-and-Hyperparameters" class="headerlink" title="Parameters and Hyperparameters"></a>Parameters and Hyperparameters</h2><h2 id="Comparation-of-shallow-and-deep-neural-network"><a href="#Comparation-of-shallow-and-deep-neural-network" class="headerlink" title="Comparation of shallow and deep neural network"></a>Comparation of shallow and deep neural network</h2>
        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2018-10-31T03:54:43.843Z" itemprop="dateUpdated">2018-10-30 23:54:43</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="https://lrscy.github.io">
            <img src="../../../../img/avatar.jpg" alt="Meow">
            Meow
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/Coursera/">Coursera</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/Deep-Learning/">Deep Learning</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/&title=《DeepLearning.ai Note - Neural Network and Deep Learning》 — Meow&pic=https://lrscy.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/&title=《DeepLearning.ai Note - Neural Network and Deep Learning》 — Meow&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《DeepLearning.ai Note - Neural Network and Deep Learning》 — Meow&url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/&via=https://lrscy.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="../../../03/20/Data-clean-for-Machine-Translation-CE/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">数据清洗工具及相关使用方法</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'false' == 'true',
            verify: 'true' == 'true',
            appId: "flgn5yN4nyhhyKPBimaT89Q8-gzGzoHsz",
            appKey: "4yw3zmlMB7KjyFIzwIlYUnKz",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->




</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        UV：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        PV：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="../../../../atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Meow &copy; 2015 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/&title=《DeepLearning.ai Note - Neural Network and Deep Learning》 — Meow&pic=https://lrscy.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/&title=《DeepLearning.ai Note - Neural Network and Deep Learning》 — Meow&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《DeepLearning.ai Note - Neural Network and Deep Learning》 — Meow&url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/&via=https://lrscy.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://lrscy.github.io/2018/10/22/DeepLearningNotes-NNandDL/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACL0lEQVR42u3aS27DMAwFwNz/0inQdeO8RyUFLI1WReqPxguCpPh4xOv5u9rfX/33+prru5YWBgbGbRnPy/Xq0ddXJuyEkewNAwPjHMbslXkMnG2xCPQYGBgYZfBNNte+FwMDA6MNuHmSl2w6/2QYGBgnM2YhMrkyZ1zf+7FaHAMD44aM9mDgP//+4vkGBgbGTRjPciUhNQ+IsyD7x10YGBhbM67bZ0npODs8SJp3SYlbbA4DA+O2jFkjrB3FmIXaYlAMAwNja8bsYGB2nJlv6Pqaj42FYWBg3JCxMrD1qeb+UjMOAwPjAEZ7PJC8rP0QSWAt2m0YGBgbMWaFazva1QboNhBjYGDsypi18vOG2sqkRF5UY2BgnMCYJWezBlnOK44QMDAwDmC0DbU2vcsZ7dgHBgbGaYyVEJlfs36c+bJfiIGBcQBjlvC1ZW0e0Ie1OAYGxqaMdniiPU6YDWTk6SkGBsYJjJXH5SNf+WFAO0CGgYGxNyNvvreJYNv0X/kFAwNjb0Ye1Ga8lcK1KHExMDAOYCTtszbsJmliDn7zZAwMjK0ZSVGaB9xZc3/lODP6KhgYGDdnPMuVp4xJ0EwSwSiUY2BgbM1oW2NtiZuPWaw8GQMD4wRGG2TzJG/lyXkBjIGBcQ5jqTUfb3oY75OiFwMDA2NE/UYx/GZmBAMDA2M05tW+azZAhoGBcQIjCY6z8Yi29Z+nnsOTWAwMjBsyZgXkSsLXtudmRTUGBsYWjB9XRPAuDEdLxgAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Where are you!';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)Hi! Welcome Back!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
